{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_dtype = torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Harvey\\anaconda3\\envs\\ml_dl\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea5b6a0392a4962aaaf60e26a518010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nijalmot/llama2-ko-dementia were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.q_proj.base_layer.weight', 'model.layers.0.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.q_proj.lora_A.default.weight', 'model.layers.0.self_attn.q_proj.lora_B.default.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight', 'model.layers.0.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.0.self_attn.v_proj.lora_A.default.weight', 'model.layers.0.self_attn.v_proj.lora_B.default.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight', 'model.layers.1.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.q_proj.lora_A.default.weight', 'model.layers.1.self_attn.q_proj.lora_B.default.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight', 'model.layers.1.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.1.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.1.self_attn.v_proj.lora_A.default.weight', 'model.layers.1.self_attn.v_proj.lora_B.default.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight', 'model.layers.10.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.q_proj.lora_A.default.weight', 'model.layers.10.self_attn.q_proj.lora_B.default.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight', 'model.layers.10.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.10.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.10.self_attn.v_proj.lora_A.default.weight', 'model.layers.10.self_attn.v_proj.lora_B.default.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight', 'model.layers.11.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.q_proj.lora_A.default.weight', 'model.layers.11.self_attn.q_proj.lora_B.default.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight', 'model.layers.11.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.11.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.11.self_attn.v_proj.lora_A.default.weight', 'model.layers.11.self_attn.v_proj.lora_B.default.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight', 'model.layers.12.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.q_proj.lora_A.default.weight', 'model.layers.12.self_attn.q_proj.lora_B.default.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight', 'model.layers.12.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.12.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.12.self_attn.v_proj.lora_A.default.weight', 'model.layers.12.self_attn.v_proj.lora_B.default.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight', 'model.layers.13.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.q_proj.lora_A.default.weight', 'model.layers.13.self_attn.q_proj.lora_B.default.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight', 'model.layers.13.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.13.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.13.self_attn.v_proj.lora_A.default.weight', 'model.layers.13.self_attn.v_proj.lora_B.default.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight', 'model.layers.14.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.q_proj.lora_A.default.weight', 'model.layers.14.self_attn.q_proj.lora_B.default.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight', 'model.layers.14.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.14.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.14.self_attn.v_proj.lora_A.default.weight', 'model.layers.14.self_attn.v_proj.lora_B.default.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight', 'model.layers.15.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.q_proj.lora_A.default.weight', 'model.layers.15.self_attn.q_proj.lora_B.default.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight', 'model.layers.15.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.15.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.15.self_attn.v_proj.lora_A.default.weight', 'model.layers.15.self_attn.v_proj.lora_B.default.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight', 'model.layers.16.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.q_proj.lora_A.default.weight', 'model.layers.16.self_attn.q_proj.lora_B.default.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight', 'model.layers.16.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.16.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.16.self_attn.v_proj.lora_A.default.weight', 'model.layers.16.self_attn.v_proj.lora_B.default.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight', 'model.layers.17.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.q_proj.lora_A.default.weight', 'model.layers.17.self_attn.q_proj.lora_B.default.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight', 'model.layers.17.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.17.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.17.self_attn.v_proj.lora_A.default.weight', 'model.layers.17.self_attn.v_proj.lora_B.default.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight', 'model.layers.18.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.q_proj.lora_A.default.weight', 'model.layers.18.self_attn.q_proj.lora_B.default.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight', 'model.layers.18.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.18.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.18.self_attn.v_proj.lora_A.default.weight', 'model.layers.18.self_attn.v_proj.lora_B.default.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight', 'model.layers.19.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.q_proj.lora_A.default.weight', 'model.layers.19.self_attn.q_proj.lora_B.default.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight', 'model.layers.19.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.19.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.19.self_attn.v_proj.lora_A.default.weight', 'model.layers.19.self_attn.v_proj.lora_B.default.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight', 'model.layers.2.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.q_proj.lora_A.default.weight', 'model.layers.2.self_attn.q_proj.lora_B.default.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight', 'model.layers.2.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.2.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.2.self_attn.v_proj.lora_A.default.weight', 'model.layers.2.self_attn.v_proj.lora_B.default.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight', 'model.layers.20.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.q_proj.lora_A.default.weight', 'model.layers.20.self_attn.q_proj.lora_B.default.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight', 'model.layers.20.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.20.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.20.self_attn.v_proj.lora_A.default.weight', 'model.layers.20.self_attn.v_proj.lora_B.default.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight', 'model.layers.21.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.q_proj.lora_A.default.weight', 'model.layers.21.self_attn.q_proj.lora_B.default.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight', 'model.layers.21.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.21.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.21.self_attn.v_proj.lora_A.default.weight', 'model.layers.21.self_attn.v_proj.lora_B.default.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight', 'model.layers.22.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.q_proj.lora_A.default.weight', 'model.layers.22.self_attn.q_proj.lora_B.default.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight', 'model.layers.22.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.22.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.22.self_attn.v_proj.lora_A.default.weight', 'model.layers.22.self_attn.v_proj.lora_B.default.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight', 'model.layers.23.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.q_proj.lora_A.default.weight', 'model.layers.23.self_attn.q_proj.lora_B.default.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight', 'model.layers.23.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.23.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.23.self_attn.v_proj.lora_A.default.weight', 'model.layers.23.self_attn.v_proj.lora_B.default.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight', 'model.layers.24.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.q_proj.lora_A.default.weight', 'model.layers.24.self_attn.q_proj.lora_B.default.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight', 'model.layers.24.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.24.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.24.self_attn.v_proj.lora_A.default.weight', 'model.layers.24.self_attn.v_proj.lora_B.default.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight', 'model.layers.25.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.q_proj.lora_A.default.weight', 'model.layers.25.self_attn.q_proj.lora_B.default.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight', 'model.layers.25.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.25.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.25.self_attn.v_proj.lora_A.default.weight', 'model.layers.25.self_attn.v_proj.lora_B.default.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight', 'model.layers.26.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.q_proj.lora_A.default.weight', 'model.layers.26.self_attn.q_proj.lora_B.default.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight', 'model.layers.26.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.26.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.26.self_attn.v_proj.lora_A.default.weight', 'model.layers.26.self_attn.v_proj.lora_B.default.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight', 'model.layers.27.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.q_proj.lora_A.default.weight', 'model.layers.27.self_attn.q_proj.lora_B.default.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight', 'model.layers.27.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.27.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.27.self_attn.v_proj.lora_A.default.weight', 'model.layers.27.self_attn.v_proj.lora_B.default.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight', 'model.layers.28.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.q_proj.lora_A.default.weight', 'model.layers.28.self_attn.q_proj.lora_B.default.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight', 'model.layers.28.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.28.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.28.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.28.self_attn.v_proj.lora_A.default.weight', 'model.layers.28.self_attn.v_proj.lora_B.default.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight', 'model.layers.29.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.q_proj.lora_A.default.weight', 'model.layers.29.self_attn.q_proj.lora_B.default.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight', 'model.layers.29.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.29.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.29.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.29.self_attn.v_proj.lora_A.default.weight', 'model.layers.29.self_attn.v_proj.lora_B.default.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight', 'model.layers.3.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.q_proj.lora_A.default.weight', 'model.layers.3.self_attn.q_proj.lora_B.default.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight', 'model.layers.3.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.3.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.3.self_attn.v_proj.lora_A.default.weight', 'model.layers.3.self_attn.v_proj.lora_B.default.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight', 'model.layers.30.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.q_proj.lora_A.default.weight', 'model.layers.30.self_attn.q_proj.lora_B.default.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight', 'model.layers.30.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.30.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.30.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.30.self_attn.v_proj.lora_A.default.weight', 'model.layers.30.self_attn.v_proj.lora_B.default.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight', 'model.layers.31.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.q_proj.lora_A.default.weight', 'model.layers.31.self_attn.q_proj.lora_B.default.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight', 'model.layers.31.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.31.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.31.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.31.self_attn.v_proj.lora_A.default.weight', 'model.layers.31.self_attn.v_proj.lora_B.default.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight', 'model.layers.4.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.q_proj.lora_A.default.weight', 'model.layers.4.self_attn.q_proj.lora_B.default.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight', 'model.layers.4.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.4.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.4.self_attn.v_proj.lora_A.default.weight', 'model.layers.4.self_attn.v_proj.lora_B.default.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight', 'model.layers.5.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.q_proj.lora_A.default.weight', 'model.layers.5.self_attn.q_proj.lora_B.default.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight', 'model.layers.5.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.5.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.5.self_attn.v_proj.lora_A.default.weight', 'model.layers.5.self_attn.v_proj.lora_B.default.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight', 'model.layers.6.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.q_proj.lora_A.default.weight', 'model.layers.6.self_attn.q_proj.lora_B.default.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight', 'model.layers.6.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.6.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.6.self_attn.v_proj.lora_A.default.weight', 'model.layers.6.self_attn.v_proj.lora_B.default.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight', 'model.layers.7.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.q_proj.lora_A.default.weight', 'model.layers.7.self_attn.q_proj.lora_B.default.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight', 'model.layers.7.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.7.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.7.self_attn.v_proj.lora_A.default.weight', 'model.layers.7.self_attn.v_proj.lora_B.default.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight', 'model.layers.8.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.q_proj.lora_A.default.weight', 'model.layers.8.self_attn.q_proj.lora_B.default.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight', 'model.layers.8.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.8.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.8.self_attn.v_proj.lora_A.default.weight', 'model.layers.8.self_attn.v_proj.lora_B.default.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight', 'model.layers.9.self_attn.q_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.q_proj.lora_A.default.weight', 'model.layers.9.self_attn.q_proj.lora_B.default.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight', 'model.layers.9.self_attn.v_proj.base_layer.weight.absmax', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_map', 'model.layers.9.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4', 'model.layers.9.self_attn.v_proj.lora_A.default.weight', 'model.layers.9.self_attn.v_proj.lora_B.default.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at nijalmot/llama2-ko-dementia and are newly initialized: ['model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'nijalmot/llama2-ko-dementia',\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "              'nijalmot/llama2-ko-dementia',\n",
    "              trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'▁ske': 18109,\n",
       " '▁맞음': 42620,\n",
       " '▁첩': 43415,\n",
       " '증권': 34666,\n",
       " '▁hand': 1361,\n",
       " '▁방송되는': 38919,\n",
       " '시스': 32972,\n",
       " '응': 45055,\n",
       " '▁partecip': 22885,\n",
       " '▁devient': 15210,\n",
       " '안이': 34334,\n",
       " '▁Sorry': 8221,\n",
       " '▁Click': 16297,\n",
       " '=\"\"': 13776,\n",
       " '▁big': 4802,\n",
       " '▁nombre': 5419,\n",
       " '▁부끄럽': 39246,\n",
       " 'nea': 14011,\n",
       " '人': 30313,\n",
       " 'Reader': 6982,\n",
       " 'hips': 14587,\n",
       " '▁다음': 32658,\n",
       " '▁_{': 7722,\n",
       " 'qual': 15380,\n",
       " '▁corte': 28776,\n",
       " '필드': 44668,\n",
       " 'gebaut': 27617,\n",
       " '▁уні': 25377,\n",
       " 'жа': 2711,\n",
       " '▁막아': 36752,\n",
       " '▁Luis': 12583,\n",
       " '▁순': 32479,\n",
       " '▁카메': 36453,\n",
       " '▁참여를': 42480,\n",
       " '했다면': 40268,\n",
       " '▁debugger': 18297,\n",
       " '▁day': 2462,\n",
       " '▁갈등': 35610,\n",
       " '▁nave': 21102,\n",
       " 'bum': 2404,\n",
       " 'naio': 15616,\n",
       " '▁se': 409,\n",
       " '▁나라의': 36564,\n",
       " 'ivent': 27649,\n",
       " 'lef': 25874,\n",
       " '▁그린': 35895,\n",
       " 'Organ': 27356,\n",
       " 'aska': 16191,\n",
       " 'pection': 27988,\n",
       " 'fection': 20309,\n",
       " '▁kra': 18858,\n",
       " 'ears': 15451,\n",
       " '쓴다': 38287,\n",
       " '▁accord': 15017,\n",
       " '▁few': 2846,\n",
       " 'the': 1552,\n",
       " '▁wohl': 25304,\n",
       " '▁mate': 15358,\n",
       " '▁교통사고': 38648,\n",
       " '▁Gol': 20268,\n",
       " 'ome': 608,\n",
       " '▁같냐': 41007,\n",
       " '▁struggling': 20042,\n",
       " '▁행사는': 42468,\n",
       " '<0xA1>': 164,\n",
       " '▁project': 2060,\n",
       " 'rikt': 27171,\n",
       " '▁sorts': 23551,\n",
       " '▁elli': 22434,\n",
       " '▁수영': 42909,\n",
       " '▁넘들': 41356,\n",
       " '▁이용해서': 42699,\n",
       " '>/': 20690,\n",
       " '▁나오': 32495,\n",
       " '▁former': 4642,\n",
       " '▁디지털': 34130,\n",
       " '▁제기': 34583,\n",
       " '깜': 45383,\n",
       " '▁득점': 36947,\n",
       " '▁threaten': 20616,\n",
       " '▁Moz': 18129,\n",
       " 'eqn': 7589,\n",
       " '▁suc': 8630,\n",
       " '▁미접종': 39809,\n",
       " 'нь': 5392,\n",
       " '▁Archite': 19255,\n",
       " '▁Leo': 22533,\n",
       " '▁Long': 6242,\n",
       " '▁줄이': 43303,\n",
       " '위가': 34123,\n",
       " 'ands': 4167,\n",
       " 'agan': 18939,\n",
       " 'zenie': 18638,\n",
       " 'ྱ': 31896,\n",
       " 'acing': 9390,\n",
       " '▁х': 4354,\n",
       " 'esp': 9983,\n",
       " '▁몰라': 36450,\n",
       " '▁Alabama': 26911,\n",
       " '▁kon': 4139,\n",
       " '▁플레이오프': 44726,\n",
       " '구려': 41124,\n",
       " '▁SW': 25289,\n",
       " '▁뛰어난': 40154,\n",
       " '▁corner': 11155,\n",
       " '장과': 34361,\n",
       " '▁Итали': 21932,\n",
       " '▁훌륭한': 39872,\n",
       " '▁phen': 17292,\n",
       " '▁contiene': 8651,\n",
       " '▁호구': 38531,\n",
       " '▁poor': 6460,\n",
       " 'äft': 14133,\n",
       " 'çoit': 28671,\n",
       " 'ident': 1693,\n",
       " '▁Pal': 3793,\n",
       " '▁사야': 42983,\n",
       " '자도': 34086,\n",
       " '▁달고': 38212,\n",
       " 'empre': 25049,\n",
       " 'ams': 2232,\n",
       " '▁wouldn': 7656,\n",
       " '▁là': 18916,\n",
       " '▁części': 24177,\n",
       " '▁시점': 39978,\n",
       " '̱': 31574,\n",
       " '▁팀': 32986,\n",
       " '▁ingles': 19350,\n",
       " ')\\\\,': 24224,\n",
       " '▁star': 5810,\n",
       " 'śc': 29259,\n",
       " '▁assumption': 11833,\n",
       " '▁뭐지': 41040,\n",
       " '<0x54>': 87,\n",
       " '▁conference': 21362,\n",
       " '▁자체가': 34367,\n",
       " '▁나무': 35708,\n",
       " '▁아쉬운': 41223,\n",
       " 'bind': 5355,\n",
       " '▁nä': 22248,\n",
       " '▁pue': 19312,\n",
       " '▁왈': 43152,\n",
       " '▁윤호중': 41505,\n",
       " '▁redund': 22275,\n",
       " '지를': 32661,\n",
       " '끌': 45206,\n",
       " '류를': 38084,\n",
       " '▁Trad': 18375,\n",
       " '▁덜': 34135,\n",
       " '▁injury': 24092,\n",
       " '“,': 9785,\n",
       " '단': 31746,\n",
       " 'Renderer': 21323,\n",
       " '▁так': 4553,\n",
       " '안을': 33260,\n",
       " '▁dost': 27113,\n",
       " 'scope': 6078,\n",
       " '▁Bür': 15455,\n",
       " 'Err': 19212,\n",
       " '▁distributed': 13235,\n",
       " '▁Error': 4829,\n",
       " '▁emb': 7232,\n",
       " 'direction': 20845,\n",
       " '▁노무현': 34680,\n",
       " '깍': 45611,\n",
       " '▁bras': 13137,\n",
       " 'abile': 18119,\n",
       " 'aring': 4362,\n",
       " '▁lég': 10918,\n",
       " '▁medic': 13589,\n",
       " '▁신뢰': 34623,\n",
       " '▁plants': 18577,\n",
       " '▁A': 319,\n",
       " '▁circuit': 11369,\n",
       " '▁aqu': 10592,\n",
       " 'nou': 22897,\n",
       " '▁phone': 9008,\n",
       " '▁assign': 3566,\n",
       " '주가': 34639,\n",
       " '▁intelligence': 21082,\n",
       " '▁dressed': 27121,\n",
       " 'ိ': 31498,\n",
       " '▁проф': 13503,\n",
       " 'Pass': 7129,\n",
       " '▁располо': 17650,\n",
       " '늪': 46173,\n",
       " '▁pels': 29549,\n",
       " '치고': 32944,\n",
       " '▁Singap': 21538,\n",
       " 'available': 16515,\n",
       " '▁Fall': 14053,\n",
       " '▁강추': 33980,\n",
       " '▁충돌': 39195,\n",
       " 'образ': 23944,\n",
       " '▁>>': 5099,\n",
       " 'aes': 28628,\n",
       " '▁Dis': 3295,\n",
       " '▁Egypt': 12892,\n",
       " '▁결국': 32833,\n",
       " '리핀': 39575,\n",
       " '▁Wort': 13824,\n",
       " '▁쾌': 38422,\n",
       " 'Digital': 27103,\n",
       " 'DI': 4571,\n",
       " '▁Und': 14211,\n",
       " '<0xD3>': 214,\n",
       " '▁ignore': 11455,\n",
       " '▁stretch': 16116,\n",
       " '<0xF4>': 247,\n",
       " '▁굴': 34868,\n",
       " '▁그러고': 40466,\n",
       " '▁Bayer': 21727,\n",
       " '▁선임': 36905,\n",
       " 'ju': 4900,\n",
       " '▁Raymond': 21380,\n",
       " '▁공작': 40184,\n",
       " '내는': 34042,\n",
       " '▁넘어가': 41981,\n",
       " '년부터': 34485,\n",
       " '면으로': 43578,\n",
       " '조는': 41919,\n",
       " 'aire': 5218,\n",
       " '욱': 45245,\n",
       " '손': 45022,\n",
       " '▁pouvoir': 19397,\n",
       " 'ola': 2963,\n",
       " 'onto': 10268,\n",
       " '▁num': 954,\n",
       " '▁electric': 12646,\n",
       " '▁vista': 21225,\n",
       " '없어서': 40309,\n",
       " 'сторія': 23548,\n",
       " '▁Kath': 19663,\n",
       " '▁acted': 27320,\n",
       " '▁tile': 25900,\n",
       " 'pgf': 14918,\n",
       " 'ждение': 27625,\n",
       " '▁альбо': 25805,\n",
       " 'рист': 28071,\n",
       " '▁사람들을': 38827,\n",
       " '▁취약계층': 39468,\n",
       " '중국': 34062,\n",
       " '▁tenía': 19987,\n",
       " '▁nearby': 20810,\n",
       " '▁Democratic': 19083,\n",
       " '▁[\"': 6796,\n",
       " '▁aston': 24293,\n",
       " 'chi': 4161,\n",
       " '▁Bridge': 16230,\n",
       " '▁още': 29521,\n",
       " 'folg': 10132,\n",
       " 'late': 9632,\n",
       " '▁Aff': 13737,\n",
       " '▁나눔': 40814,\n",
       " '▁자식이': 43376,\n",
       " '폐지': 43614,\n",
       " '▁citt': 10439,\n",
       " '▁쪽팔': 36896,\n",
       " '▁부사': 44669,\n",
       " '▁także': 16331,\n",
       " 'ListItem': 27490,\n",
       " 'доступ': 29033,\n",
       " '▁challenge': 18766,\n",
       " '▁estre': 24567,\n",
       " 'onnées': 14201,\n",
       " '▁seven': 9881,\n",
       " '▁suff': 9378,\n",
       " 'alias': 19973,\n",
       " '▁조국수호': 44798,\n",
       " '▁allo': 25169,\n",
       " '▁방법은': 44108,\n",
       " 'ouwd': 26888,\n",
       " '▁hous': 9261,\n",
       " 'umin': 9735,\n",
       " '▁calculated': 12833,\n",
       " 'Pad': 20369,\n",
       " '▁환자': 35604,\n",
       " 'armée': 22413,\n",
       " '▁pier': 9307,\n",
       " '버려야': 44550,\n",
       " '진한': 43672,\n",
       " 'iegel': 19369,\n",
       " '▁criminal': 22161,\n",
       " 'Res': 1666,\n",
       " 'alous': 20521,\n",
       " 'xxxx': 14633,\n",
       " 'zat': 22046,\n",
       " '▁Fritz': 22839,\n",
       " '▁Quando': 26425,\n",
       " '▁Vue': 22518,\n",
       " 'prim': 9469,\n",
       " '▁OCLC': 22610,\n",
       " '▁Yang': 27583,\n",
       " '▁characters': 4890,\n",
       " '▁diverses': 27858,\n",
       " '▁enabled': 9615,\n",
       " '▁slov': 25007,\n",
       " '▁Übers': 19999,\n",
       " '▁고맙다': 41375,\n",
       " 'പ': 30707,\n",
       " '▁꾸준히': 36293,\n",
       " '▁발급': 39913,\n",
       " '하지말고': 35070,\n",
       " '▁pel': 4639,\n",
       " '▁노하': 42159,\n",
       " '▁practices': 23274,\n",
       " '▁carried': 8988,\n",
       " '▁베트남': 35993,\n",
       " 'Age': 22406,\n",
       " '▁ind': 1399,\n",
       " '▁석': 33815,\n",
       " '실히': 39895,\n",
       " '▁absolutely': 13312,\n",
       " 'zyma': 27425,\n",
       " '주신': 44500,\n",
       " 'su': 2146,\n",
       " '▁일정을': 39731,\n",
       " '○': 31236,\n",
       " '例': 31507,\n",
       " 'rightarrow': 5211,\n",
       " '▁이미지를': 43752,\n",
       " '▁damit': 14733,\n",
       " 'entr': 14856,\n",
       " 'username': 6786,\n",
       " '▁f': 285,\n",
       " '▁terre': 18249,\n",
       " 'fragment': 20777,\n",
       " 'urm': 17095,\n",
       " '▁국회에': 44418,\n",
       " '▁벗어나': 37483,\n",
       " '▁아르헨티나': 44105,\n",
       " '단과': 44144,\n",
       " '을까': 32908,\n",
       " '▁ensemble': 21285,\n",
       " '▁공약을': 43124,\n",
       " '▁본인의': 38938,\n",
       " '▁운영한다': 43914,\n",
       " '걀': 46142,\n",
       " 'lp': 22833,\n",
       " 'erem': 14992,\n",
       " '취': 45062,\n",
       " '▁id': 1178,\n",
       " '▁ui': 14313,\n",
       " 'opus': 26466,\n",
       " '▁Apple': 12113,\n",
       " '▁구매했는데': 37271,\n",
       " '▁bul': 8227,\n",
       " '▁\\\\,': 7179,\n",
       " 'loading': 13234,\n",
       " '▁rect': 7705,\n",
       " '거리가': 39712,\n",
       " 'cer': 2265,\n",
       " '▁Нов': 27527,\n",
       " 'Entry': 9634,\n",
       " '▁오픈': 35207,\n",
       " '▁adopted': 16356,\n",
       " '▁속이': 38736,\n",
       " '▁품': 33197,\n",
       " 'endet': 16877,\n",
       " '▁Kub': 29024,\n",
       " '▁юго': 28162,\n",
       " '▁데': 32392,\n",
       " 'ansen': 29401,\n",
       " '었는데': 33189,\n",
       " '▁Shaw': 28548,\n",
       " '▁징': 33056,\n",
       " '▁Prof': 6175,\n",
       " 'igned': 12961,\n",
       " '▁구축': 33648,\n",
       " '▁basic': 6996,\n",
       " '▁plugin': 7079,\n",
       " 'ECT': 13845,\n",
       " '▁재활': 37945,\n",
       " '화를': 32835,\n",
       " 'tatywna': 20486,\n",
       " 'шення': 25093,\n",
       " '스러운': 34689,\n",
       " '▁manip': 11525,\n",
       " '▁선보이는': 44237,\n",
       " 'vest': 10147,\n",
       " 'Found': 9692,\n",
       " '▁pir': 21625,\n",
       " 'anh': 27731,\n",
       " '▁Binding': 25799,\n",
       " '▁Glas': 19286,\n",
       " '▁ihrer': 11448,\n",
       " '▁shadow': 15504,\n",
       " '▁지금이라도': 37870,\n",
       " '무': 31716,\n",
       " 'catal': 16431,\n",
       " 'court': 27845,\n",
       " 'при': 7104,\n",
       " 'Well': 11284,\n",
       " 'бор': 7677,\n",
       " '▁siehe': 27005,\n",
       " '▁설정': 40953,\n",
       " 'apor': 26191,\n",
       " 'rita': 27250,\n",
       " '▁구제': 44504,\n",
       " '=': 29922,\n",
       " 'Mapping': 15845,\n",
       " '▁밝혔': 32277,\n",
       " '▁FILE': 24080,\n",
       " '▁Lew': 11906,\n",
       " '▁apply': 3394,\n",
       " '▁unfortunately': 15428,\n",
       " 'rai': 17016,\n",
       " '▁début': 13222,\n",
       " '종일': 43114,\n",
       " '▁Cir': 25079,\n",
       " ')': 29897,\n",
       " 'Constraint': 21529,\n",
       " 'ix': 861,\n",
       " '▁board': 7613,\n",
       " '▁repr': 2062,\n",
       " '▁제정': 40971,\n",
       " 'nosti': 13887,\n",
       " 'ַ': 30634,\n",
       " '▁moreover': 25409,\n",
       " 'Day': 12742,\n",
       " '▁rice': 19408,\n",
       " '▁보여': 32731,\n",
       " 'CR': 11341,\n",
       " 'ovi': 6895,\n",
       " 'дон': 19096,\n",
       " 'ulty': 18857,\n",
       " '민이': 36720,\n",
       " '▁Deb': 7089,\n",
       " '▁구글': 39469,\n",
       " '장으로': 34267,\n",
       " '찾': 45160,\n",
       " 'only': 6194,\n",
       " 'controller': 8299,\n",
       " '▁auxili': 29587,\n",
       " '▁pens': 10420,\n",
       " '뎁': 46271,\n",
       " '▁올해는': 38011,\n",
       " '▁etwa': 10218,\n",
       " '▁그럼': 32792,\n",
       " '▁채워': 41674,\n",
       " '▁내용은': 38258,\n",
       " '난다': 33626,\n",
       " '중계': 41783,\n",
       " '▁мето': 20148,\n",
       " '▁Vas': 15453,\n",
       " '닙': 45519,\n",
       " '▁soir': 29627,\n",
       " '▁subjects': 17800,\n",
       " '▁;)': 15718,\n",
       " 'цер': 17267,\n",
       " '▁track': 5702,\n",
       " '▁맡고': 42925,\n",
       " '▁structures': 12286,\n",
       " '▁жен': 14244,\n",
       " '▁마땅': 39276,\n",
       " '▁nú': 10442,\n",
       " '▁수소': 38674,\n",
       " '▁해서': 33227,\n",
       " '（': 30419,\n",
       " '▁samt': 20642,\n",
       " '▁society': 12459,\n",
       " '▁extra': 4805,\n",
       " '<0x26>': 41,\n",
       " '▁폐': 32693,\n",
       " 'вид': 20874,\n",
       " 'ня': 1200,\n",
       " 'бір': 20257,\n",
       " '▁멀쩡': 37301,\n",
       " '▁어떨': 39581,\n",
       " '▁행태': 38460,\n",
       " '▁((': 5135,\n",
       " '▁Ain': 23719,\n",
       " '▁SA': 16698,\n",
       " '▁stays': 27111,\n",
       " '▁conc': 3022,\n",
       " '▁기쁨': 41771,\n",
       " 'ición': 6396,\n",
       " 'indows': 2054,\n",
       " 'typename': 22646,\n",
       " '▁K': 476,\n",
       " '▁incorrectly': 29676,\n",
       " 'ос': 3264,\n",
       " '누가': 39057,\n",
       " '브랜드': 42431,\n",
       " '▁tr': 534,\n",
       " '▁장관이': 37993,\n",
       " '▁달라고': 38878,\n",
       " 'ners': 8397,\n",
       " '▁ter': 1935,\n",
       " '▁누구보다': 44236,\n",
       " '▁말까지': 41695,\n",
       " '▁questa': 15430,\n",
       " 'mai': 24402,\n",
       " '▁따뜻한': 38534,\n",
       " '▁민주당에서': 43937,\n",
       " '▁앉': 33873,\n",
       " '▁матери': 20649,\n",
       " 'хи': 3899,\n",
       " '▁Lord': 6171,\n",
       " '▁był': 11573,\n",
       " '▁combine': 14405,\n",
       " '였지만': 41760,\n",
       " 'unque': 11246,\n",
       " '▁부자': 36966,\n",
       " '▁겁니다': 35334,\n",
       " 'Args': 7883,\n",
       " '▁CP': 28505,\n",
       " '▁Holland': 20262,\n",
       " '▁dise': 10267,\n",
       " \"').\": 2824,\n",
       " 'া': 30445,\n",
       " '▁리뷰': 39646,\n",
       " '4': 29946,\n",
       " 'vity': 17037,\n",
       " '▁Hand': 5166,\n",
       " '▁Marina': 27169,\n",
       " '팰': 46161,\n",
       " '九': 31321,\n",
       " '本': 30346,\n",
       " '이에요': 35739,\n",
       " '제의': 39819,\n",
       " 'osp': 4705,\n",
       " '\":': 1115,\n",
       " '▁Unicode': 23862,\n",
       " '▁Mal': 3792,\n",
       " '▁css': 5997,\n",
       " '▁battle': 10555,\n",
       " '▁할듯': 41499,\n",
       " '▁고용': 34495,\n",
       " '궁': 45251,\n",
       " '▁being': 1641,\n",
       " '▁dialect': 23725,\n",
       " '▁Према': 19196,\n",
       " '▁Bib': 19864,\n",
       " '▁Singapore': 25960,\n",
       " 'kal': 11311,\n",
       " '▁entstand': 16875,\n",
       " '▁수면': 44139,\n",
       " '▁Méd': 26402,\n",
       " '▁시기': 37224,\n",
       " '▁있는거': 37339,\n",
       " '▁장을': 41204,\n",
       " '勝': 31721,\n",
       " '왔고': 44411,\n",
       " 'atel': 14830,\n",
       " 'verte': 25996,\n",
       " '▁easier': 6775,\n",
       " '▁NSLog': 20384,\n",
       " 'guard': 17728,\n",
       " 'versary': 27547,\n",
       " '▁mot': 3184,\n",
       " '▁look': 1106,\n",
       " 'ensor': 6073,\n",
       " '▁Gate': 22510,\n",
       " '넹': 45930,\n",
       " '▁Apache': 13380,\n",
       " '▁마구': 41075,\n",
       " '▁Vitt': 29364,\n",
       " '▁Prince': 10787,\n",
       " 'NER': 13865,\n",
       " '▁же': 5865,\n",
       " '바다': 38141,\n",
       " '▁exterior': 25591,\n",
       " '▁CS': 21107,\n",
       " '()}': 28296,\n",
       " '▁gained': 17515,\n",
       " '▁Gew': 14412,\n",
       " '인간': 33631,\n",
       " \"'\\\\\": 12764,\n",
       " '▁directly': 4153,\n",
       " 'module': 5453,\n",
       " '▁해준': 44656,\n",
       " 'ände': 14362,\n",
       " '▁적어도': 38256,\n",
       " '▁슬프': 42377,\n",
       " '▁те': 2399,\n",
       " '▁cols': 28730,\n",
       " 'BASE': 25416,\n",
       " '▁한달': 35732,\n",
       " '\\\\{\\\\': 24976,\n",
       " 'network': 11618,\n",
       " '居': 31924,\n",
       " '▁자금': 36183,\n",
       " 'estr': 16444,\n",
       " '▁unnecessary': 19039,\n",
       " '▁Ever': 18274,\n",
       " '：': 30383,\n",
       " '웨이': 38498,\n",
       " '하이': 35945,\n",
       " 'вала': 14906,\n",
       " '▁програм': 20796,\n",
       " '▁가나': 37828,\n",
       " '▁keep': 3013,\n",
       " 'DA': 7698,\n",
       " 'Hrsg': 10196,\n",
       " '▁jquery': 5804,\n",
       " '▁Elle': 6340,\n",
       " '▁lowest': 19604,\n",
       " 'сс': 8808,\n",
       " '▁invece': 24248,\n",
       " '▁Pick': 23868,\n",
       " 'oin': 28230,\n",
       " 'orter': 9555,\n",
       " '▁리얼': 38252,\n",
       " 'Completed': 26010,\n",
       " '▁휘발': 41127,\n",
       " '일이다': 42789,\n",
       " '▁fairly': 12558,\n",
       " '▁로스': 44197,\n",
       " '▁기상': 37577,\n",
       " 'yman': 21909,\n",
       " '▁Гре': 27174,\n",
       " '▁머스크': 38940,\n",
       " '▁이쁘고': 35081,\n",
       " 'gate': 17062,\n",
       " 'imation': 7715,\n",
       " '는': 31081,\n",
       " '알바': 40927,\n",
       " 'new': 1482,\n",
       " 'path': 2084,\n",
       " '▁acts': 14741,\n",
       " '▁죽일': 38914,\n",
       " 'Token': 6066,\n",
       " '▁beat': 16646,\n",
       " '▁잣': 40117,\n",
       " 'лением': 27414,\n",
       " 'unos': 12609,\n",
       " '▁Tools': 27564,\n",
       " '▁Santi': 15908,\n",
       " '▁뚜껑': 42486,\n",
       " '으로써': 36503,\n",
       " 'aille': 8284,\n",
       " '▁Jewish': 16728,\n",
       " '▁Referencias': 6966,\n",
       " 'Flow': 17907,\n",
       " '▁무기': 35299,\n",
       " '▄': 30625,\n",
       " '레이드': 39128,\n",
       " '최': 44885,\n",
       " '▁인기': 35830,\n",
       " 'Machine': 29076,\n",
       " '▁prep': 8273,\n",
       " 'RI': 3960,\n",
       " 'тие': 18380,\n",
       " '▁pro': 410,\n",
       " 'actly': 23617,\n",
       " '}.': 1836,\n",
       " '▁Ä': 11585,\n",
       " '▁divide': 16429,\n",
       " '▁즈': 43459,\n",
       " '▁플랫폼을': 43848,\n",
       " '▁britann': 14629,\n",
       " '아니고': 37608,\n",
       " '었지': 37051,\n",
       " 'گ': 30421,\n",
       " 'зни': 14489,\n",
       " 'Double': 11843,\n",
       " 'rust': 23575,\n",
       " '▁largo': 21793,\n",
       " '▁opt': 3523,\n",
       " '포를': 39372,\n",
       " 'emes': 13826,\n",
       " '<0xCA>': 205,\n",
       " '▁Ward': 21910,\n",
       " '▁\"$': 3908,\n",
       " '유플러스': 43381,\n",
       " 'Do': 6132,\n",
       " '▁Kind': 13187,\n",
       " '되길': 40858,\n",
       " 'OM': 6488,\n",
       " 'même': 18202,\n",
       " '▁Flash': 21967,\n",
       " '▁бра': 12224,\n",
       " '고있네': 40911,\n",
       " 'rome': 4871,\n",
       " '▁road': 6520,\n",
       " '소리': 32539,\n",
       " '▁팬들이': 44143,\n",
       " '▁홈페이지': 35440,\n",
       " '▁얹': 42442,\n",
       " '▁encountered': 18169,\n",
       " '▁consp': 21588,\n",
       " '▁아직도': 33015,\n",
       " '≡': 30616,\n",
       " 'Push': 27031,\n",
       " 'BUG': 11578,\n",
       " '▁cli': 9335,\n",
       " '▁Украи': 19189,\n",
       " '▁만들어서': 37356,\n",
       " '▁모니터': 39368,\n",
       " '▁schedule': 20410,\n",
       " 'iterator': 17609,\n",
       " '▁만약': 36070,\n",
       " 'レ': 30420,\n",
       " 'nych': 5909,\n",
       " '▁President': 7178,\n",
       " '▁és': 2465,\n",
       " '▁borders': 28199,\n",
       " 'ологи': 17060,\n",
       " 'opsis': 15368,\n",
       " '▁Filter': 19916,\n",
       " '▁François': 13645,\n",
       " '▁측근': 38973,\n",
       " '▁친': 32347,\n",
       " '▁Slo': 16275,\n",
       " '▁accused': 28886,\n",
       " '▁dynam': 4292,\n",
       " '▁경찰': 32470,\n",
       " '▁Guinea': 29252,\n",
       " '▁tired': 23407,\n",
       " '▁Ні': 23865,\n",
       " '▁연기를': 44352,\n",
       " '▁휴대전': 43064,\n",
       " 'stellung': 11550,\n",
       " 'ète': 12973,\n",
       " '▁school': 3762,\n",
       " '▁구현': 38400,\n",
       " '▁Carl': 8965,\n",
       " 'ped': 9795,\n",
       " '판을': 38113,\n",
       " '▁cases': 4251,\n",
       " '▁right': 1492,\n",
       " '▁기쁘': 39823,\n",
       " '▁생각도': 39040,\n",
       " '품의': 41326,\n",
       " 'estamp': 7416,\n",
       " '▁한심': 33229,\n",
       " '춰': 45386,\n",
       " '▁아부': 40461,\n",
       " '▁Imperial': 21080,\n",
       " '셜': 45576,\n",
       " '▁showing': 6445,\n",
       " '▁dependence': 26307,\n",
       " 'ña': 9658,\n",
       " '▁Compos': 24497,\n",
       " '▁Eug': 20891,\n",
       " '윤석열': 35677,\n",
       " '딧': 45986,\n",
       " '▁인민': 37410,\n",
       " '▁pygame': 22028,\n",
       " '▁가고': 36081,\n",
       " 'iche': 4070,\n",
       " '▁NA': 8598,\n",
       " '▁zat': 29052,\n",
       " '張': 31504,\n",
       " '당에': 35761,\n",
       " 'irement': 19211,\n",
       " '▁능력을': 39419,\n",
       " 'cible': 15520,\n",
       " 'clear': 8551,\n",
       " '▁efforts': 14231,\n",
       " '▁help': 1371,\n",
       " '인것': 41111,\n",
       " '▁locate': 26694,\n",
       " '▁공무원': 33296,\n",
       " '▁항상': 33323,\n",
       " '▁미국도': 43771,\n",
       " '굉장': 36981,\n",
       " 'race': 25525,\n",
       " 'bytes': 13193,\n",
       " '후보': 33672,\n",
       " 'villa': 28765,\n",
       " '▁belong': 6852,\n",
       " '▁przed': 13952,\n",
       " 'ody': 1486,\n",
       " 'тра': 6711,\n",
       " 'Replace': 20083,\n",
       " '▁compens': 22874,\n",
       " '▁Ltd': 19806,\n",
       " '▁안하': 42341,\n",
       " '▁Mountains': 28418,\n",
       " '▁원활': 41107,\n",
       " '▁자료': 36049,\n",
       " 'Items': 6913,\n",
       " '▁suppress': 21301,\n",
       " '▁로이터': 40912,\n",
       " '렬한': 39798,\n",
       " '유산': 42188,\n",
       " 'phy': 11461,\n",
       " '질도': 38873,\n",
       " '첨': 45380,\n",
       " '쾅': 45977,\n",
       " '교체': 34085,\n",
       " '▁늘어나는': 44421,\n",
       " '房': 31975,\n",
       " '치로': 38007,\n",
       " '▁발생했다': 37852,\n",
       " '낮': 45254,\n",
       " 'vm': 6925,\n",
       " '▁Voor': 29753,\n",
       " '▁start': 1369,\n",
       " '쓱': 46048,\n",
       " 'ha': 2350,\n",
       " '▁때려': 35884,\n",
       " '▁additional': 5684,\n",
       " 'fil': 1777,\n",
       " '▁remove': 3349,\n",
       " '▁더불': 32838,\n",
       " 'kup': 22947,\n",
       " 'cript': 924,\n",
       " '▁Sur': 6298,\n",
       " '▁journ': 21824,\n",
       " '▁Ham': 7904,\n",
       " '▁aproxim': 22422,\n",
       " '▁russe': 28280,\n",
       " '▁Arten': 22335,\n",
       " '했지만': 33447,\n",
       " '▁Germany': 9556,\n",
       " '행동': 40889,\n",
       " '>\"': 11903,\n",
       " 'Groups': 24020,\n",
       " '▁Rad': 4957,\n",
       " '▁médec': 25974,\n",
       " '▁구독': 39333,\n",
       " 'cem': 19335,\n",
       " 'ientras': 15054,\n",
       " 'жде': 8558,\n",
       " '▁needs': 4225,\n",
       " 'ген': 11933,\n",
       " '▁인간을': 41387,\n",
       " '낭비': 36170,\n",
       " '▁vessels': 24479,\n",
       " '▁치는': 41050,\n",
       " 'ception': 1441,\n",
       " '▁recovery': 24205,\n",
       " 'ggi': 11832,\n",
       " '▁voice': 7314,\n",
       " '이한': 42310,\n",
       " '▁현재까지': 38947,\n",
       " 'ě': 30036,\n",
       " '▁St': 624,\n",
       " '▁perl': 21185,\n",
       " 'simple': 12857,\n",
       " '▁한자리': 41117,\n",
       " 'Listener': 3962,\n",
       " '열': 44912,\n",
       " '▁맞지': 35834,\n",
       " '($(': 26237,\n",
       " 'dates': 15190,\n",
       " '▁Zwischen': 24527,\n",
       " '▁괜찮': 32513,\n",
       " '▁맞이': 38270,\n",
       " 'spot': 17500,\n",
       " 'ор': 1419,\n",
       " '▁Commonwealth': 27094,\n",
       " 'Cy': 29733,\n",
       " '▁experience': 7271,\n",
       " '▁맞서': 41184,\n",
       " '∷': 31766,\n",
       " '▁삼성전': 37943,\n",
       " '▁싫': 32763,\n",
       " '▁재택': 38901,\n",
       " '@\",': 24695,\n",
       " '르트': 43242,\n",
       " '▁vess': 12950,\n",
       " '▁걸로': 39011,\n",
       " '버그': 41403,\n",
       " '왔던': 41575,\n",
       " '원과': 36520,\n",
       " '▁Santa': 7510,\n",
       " '금리': 34601,\n",
       " '▁Voll': 29468,\n",
       " '▁generalized': 28803,\n",
       " '▁misma': 22575,\n",
       " '▁뒤를': 41076,\n",
       " '배를': 40391,\n",
       " '▁싱글': 37505,\n",
       " 'vl': 20901,\n",
       " '건으로': 39179,\n",
       " '▁секре': 26198,\n",
       " '▁address': 3211,\n",
       " ')^{': 8940,\n",
       " '▁마다': 43132,\n",
       " '▁재활용': 40869,\n",
       " 'nym': 9574,\n",
       " '▁soc': 17874,\n",
       " '이었': 33059,\n",
       " '▁Toast': 15891,\n",
       " '▁이다': 37606,\n",
       " 'gså': 26711,\n",
       " 'ént': 29463,\n",
       " 'pel': 13111,\n",
       " 'assert': 9294,\n",
       " '▁august': 15251,\n",
       " '▁elegant': 19232,\n",
       " '▁De': 897,\n",
       " 'ês': 8769,\n",
       " '미디어': 38630,\n",
       " '▁그런지': 39415,\n",
       " 'Timer': 14745,\n",
       " '심을': 34096,\n",
       " 'iter': 1524,\n",
       " 'ær': 17930,\n",
       " '▁quarter': 12616,\n",
       " '▁propri': 10850,\n",
       " '▁sections': 13926,\n",
       " '▁тако': 8826,\n",
       " '앨': 45444,\n",
       " 'sha': 17051,\n",
       " 'щи': 2837,\n",
       " 'awk': 20011,\n",
       " 'ível': 24747,\n",
       " '▁너희들이': 41914,\n",
       " 'erva': 25461,\n",
       " '▁dut': 9379,\n",
       " 'ació': 12076,\n",
       " '▁Papa': 27412,\n",
       " '인수': 38121,\n",
       " '▁비교적': 44662,\n",
       " '▁certain': 3058,\n",
       " '▁city': 4272,\n",
       " 'mathbb': 1995,\n",
       " 'ший': 11941,\n",
       " '▁світ': 29597,\n",
       " 'TRUE': 20652,\n",
       " '▁França': 29253,\n",
       " '▁적당하고': 39467,\n",
       " '기들': 38315,\n",
       " '융합': 38849,\n",
       " 'ональ': 7170,\n",
       " '▁어쩌다': 41465,\n",
       " '▁접어': 42965,\n",
       " 'рі': 3020,\n",
       " '▁Хар': 20297,\n",
       " '▁det': 1439,\n",
       " '.*;': 26355,\n",
       " '▁post': 1400,\n",
       " '이다': 32044,\n",
       " '▁statements': 9506,\n",
       " '▁없네': 34603,\n",
       " 'atures': 3698,\n",
       " 'ike': 9345,\n",
       " '▁Hook': 29612,\n",
       " '▁beneath': 19540,\n",
       " '▁Mobile': 21600,\n",
       " '▁équipe': 25740,\n",
       " '계에': 35059,\n",
       " 'Wait': 15716,\n",
       " 'istischen': 23426,\n",
       " '스에서': 40487,\n",
       " '▁hear': 8293,\n",
       " 'lying': 5890,\n",
       " '▁England': 5408,\n",
       " 'хар': 26578,\n",
       " '▁HD': 18435,\n",
       " '▁putting': 10594,\n",
       " '▁후보를': 38450,\n",
       " 'nodes': 18010,\n",
       " '▁말이냐': 44142,\n",
       " '헉': 46076,\n",
       " 'AtIndex': 16686,\n",
       " 'MI': 10403,\n",
       " 'áb': 16341,\n",
       " '▁Chart': 14477,\n",
       " '▁carriage': 23840,\n",
       " 'enses': 11259,\n",
       " '▁third': 4654,\n",
       " '▁rôle': 17889,\n",
       " '格': 31168,\n",
       " 'itor': 2105,\n",
       " 'through': 20678,\n",
       " '▁самы': 24093,\n",
       " '▁оста': 14072,\n",
       " 'bid': 23883,\n",
       " 'ское': 7757,\n",
       " '▁dependent': 14278,\n",
       " '겪': 45459,\n",
       " '림이': 42798,\n",
       " '졌는데': 39621,\n",
       " '피언': 35296,\n",
       " '▁strugg': 10205,\n",
       " 'player': 9106,\n",
       " 'rx': 17697,\n",
       " '▁Log': 4522,\n",
       " '▁jak': 12979,\n",
       " '▁mysql': 5749,\n",
       " 'し': 30326,\n",
       " '▁Lou': 4562,\n",
       " '파트': 32726,\n",
       " 'Left': 8091,\n",
       " '해온': 38689,\n",
       " '▁heav': 14200,\n",
       " '▁일을': 34273,\n",
       " '▁Release': 23708,\n",
       " 'Þ': 30452,\n",
       " '▁Gal': 5208,\n",
       " 'uted': 3860,\n",
       " '▁않았고': 43053,\n",
       " '▁ло': 13860,\n",
       " '▁couple': 7303,\n",
       " '▁업무협약': 42532,\n",
       " 'nahme': 18692,\n",
       " '▁권력을': 37942,\n",
       " '지도': 32521,\n",
       " 'isch': 783,\n",
       " '▁Ole': 26132,\n",
       " '▁alten': 25920,\n",
       " '▁vin': 13848,\n",
       " '▁나와도': 42861,\n",
       " '약을': 33397,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] 알츠하이머병을 집에서 치료하기 위해 집에서 실천할 수 있는 가장 효과적인 방법은 무엇인가요? [/INST]\n",
      "generate time :  21.7260\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "prompt = \"알츠하이머병을 집에서 치료하기 위해 집에서 실천할 수 있는 가장 효과적인 방법은 무엇인가요?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=300, truncation=True)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "now = time.time()\n",
    "print(result[0]['generated_text'])\n",
    "print(f\"generate time : {now - start: .4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "ml_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
